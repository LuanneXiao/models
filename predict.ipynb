{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "from skimage import transform\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random;\n",
    "import math;\n",
    "from PIL import Image\n",
    "import torch.onnx as torch_onnx\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型选择之前预测正确率最高的WRN40-4（dropout=0.5)模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##WRN40-4 （dropout=0.5)模型\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv0 = nn.Conv2d(1,3,kernel_size=1,stride=1)   ##\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入已经训练好的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "cnn = WideResNet(40, 10, 4, 0.5)\n",
    "cnn.load_state_dict(torch.load('net_paramet_WRN_normalization.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入自定义图像，图像文件夹路径可更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 2 (C:/Users/15273/Desktop/figure\\fig1.jpg)\n",
      "Processing 2 of 2 (C:/Users/15273/Desktop/figure\\fig2.jpg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize(29),  #将图像的短边缩放成29个像素，长边按短边比例缩放\n",
    "    transforms.RandomCrop(28),  #随机裁剪成28*28\n",
    "    transforms.ToTensor(), \n",
    "     transforms.Normalize(mean=[.2861], std=[.3530])  #mean和std采用与训练WRN网络时相同的取值\n",
    "    ])\n",
    "def get_files(directory):\n",
    "    return [os.path.join(directory, f) for f in sorted(list(os.listdir(directory)))\n",
    "            if os.path.isfile(os.path.join(directory, f))]\n",
    "images = np.array([])\n",
    "file = get_files('C:/Users/15273/Desktop/figure')\n",
    "for i, item in enumerate(file):\n",
    "    print('Processing %i of %i (%s)' % (i+1, len(file), item))\n",
    "    image = transform(PIL.ImageOps.invert(Image.open(item).convert('L')))\n",
    "    images = np.append(images, image.numpy())        \n",
    "img = images.reshape(-1, 1, 28, 28)\n",
    "img = torch.from_numpy(img).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示输入的自定义图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize(29),  #将图像的短边缩放成29个像素，长边按短边比例缩放\n",
    "    transforms.RandomCrop(28),  #随机裁剪成28*28\n",
    "    ])\n",
    "def get_files(directory):\n",
    "    return [os.path.join(directory, f) for f in sorted(list(os.listdir(directory)))\n",
    "            if os.path.isfile(os.path.join(directory, f))]\n",
    "images = np.array([])\n",
    "file1 = get_files('C:/Users/15273/Desktop/figure')\n",
    "for i, item in enumerate(file1):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    image = transform(Image.open(item))\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对图像进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  Sandal Trouser\n"
     ]
    }
   ],
   "source": [
    "outputs = cnn(img)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "classes = ('T-shirt', 'Trouser', 'Pollover', 'Dress',\n",
    "           'Coat', 'Sandal', 'Shirt','Sneaker', 'Bag', 'Ankle boot')\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
